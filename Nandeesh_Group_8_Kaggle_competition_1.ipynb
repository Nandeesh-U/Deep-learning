{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nandeesh-U/Deep-learning/blob/main/Nandeesh_Group_8_Kaggle_competition_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "cXF2o-eYe5og"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FL0Ve1abn6YJ"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "The challenge in this competition is to predict whether a question asked on a well known public forum/platform is Toxic/inappropriate or not.\n",
        "\n",
        "A toxic/inappropriate question is defined as a question intended to make a statement and not with a purpose of looking for helpful/meaningful answers. The following are some of the characteristics that can signify that a question is irrelevant/inappropriate:\n",
        "\n",
        "* Based on false information, or contains absurd assumptions\n",
        "* Does not have a non-neutral tone\n",
        "* Has an exaggerated tone to underscore a point about a group of people\n",
        "* Is rhetorical and meant to imply a statement about a group of people\n",
        "* Is disparaging or inflammatory against an individual or a group of people\n",
        "* Uses sexual content (such as incest, pedophilia), and not to seek genuine answers\n",
        "* Suggests a discriminatory idea against a protected class of people, or seeks confirmation of a stereotype\n",
        "* Based on an unrealistic premise about a group of people\n",
        "* Is not grounded in reality\n",
        "\n",
        "The training dataset includes the questions 1044897 that was asked, and whether it was identified as toxic/inappropriate (target = 1) or as relevant/appropriate (target = 0). The test dataset consists of approximately 261000 questions.\n",
        "\n",
        "The training data might be imbalanced or noisy. They are not guaranteed to be perfect. Please take the necessary actions/steps while building the model.\n",
        "\n",
        "\n",
        "## Description\n",
        "\n",
        "This dataset has the following information:\n",
        "\n",
        "1. **qid** - unique question identifier\n",
        "2. **question_text** - the text of the question asked in the well known public forum/platform\n",
        "3. **target** - a question labeled \"toxic/inappropriate\" has a value of 1, otherwise 0\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ih-oasWmdZul"
      },
      "source": [
        "## Problem Statement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfWGmjNHdZul"
      },
      "source": [
        "To perform classification of approximately 261000 questions asked on a well known public form using Deep Neural Networks such as RNN/CNN/BERT/LSTM as 'toxic/inappropriate' questions or 'relevant/appropriate' questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtETuXx8b-OC"
      },
      "source": [
        "### 3. Upload your kaggle.json file using the following snippet in a code cell:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1pfXBDxWl0Y"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCV_T6MMW4eX"
      },
      "source": [
        "#If successfully uploaded in the above step, the 'ls' command here should display the kaggle.json file.\n",
        "%ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbukdzJ6cE32"
      },
      "source": [
        "### 4. Install the Kaggle API using the following command\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install urllib3"
      ],
      "metadata": {
        "id": "G5Dvw5EJnC0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMj1n1MJcqzN"
      },
      "source": [
        "#!pip install -U -q kaggle==1.5.8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Vpy9P1nchhd"
      },
      "source": [
        "### 5. Move the kaggle.json file into ~/.kaggle, which is where the API client expects your token to be located:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQbPsDOLZ0b4"
      },
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BenAWlpI73sm"
      },
      "source": [
        "# Execute the following command to verify whether the kaggle.json is stored in the appropriate location: ~/.kaggle/kaggle.json\n",
        "!ls ~/.kaggle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vm2jGsCradOS"
      },
      "source": [
        "!chmod 600 /root/.kaggle/kaggle.json # run this command to ensure your Kaggle API token is secure on colab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32unPZKzdI72"
      },
      "source": [
        "### 6. Now download the Test Data from Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TY40TmgfHq0s"
      },
      "source": [
        "#If you get a forbidden link, you have most likely not joined the competition.\n",
        "!kaggle competitions download -c irrelevant-questions-classification"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''import zipfile\n",
        "zip_ref = zipfile.ZipFile('/content/irrelevant-questions-classification.zip', 'r')\n",
        "zip_ref.extractall('/content/drive/MyDrive/colab_data_files/kaggle_1_dataset')\n",
        "zip_ref.close()'''"
      ],
      "metadata": {
        "id": "W7bTnUlm2YBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/irrelevant-questions-classification.zip"
      ],
      "metadata": {
        "id": "mvKRiFNglvpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install transformers"
      ],
      "metadata": {
        "id": "NDHRUT5CFrzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.utils import shuffle\n",
        "import tensorflow as tf\n",
        "import nltk\n",
        "from sklearn.metrics import f1_score\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from keras.layers import Input, Embedding, Dense, Bidirectional, Dropout, GRU, LSTM\n",
        "from keras.models import Sequential   # the model\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "btM-2eBCIgJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53g0zVbjRV7K"
      },
      "source": [
        "##   **Stage 1**:  Data Loading and Perform Exploratory Data Analysis (1 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracting data from a stored folder to maintain consistency across the project"
      ],
      "metadata": {
        "id": "XyDyGyQ01PXg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pd.read_csv(\"/content/drive/MyDrive/colab_data_files/kaggle_1_dataset/train_dataset.csv\")\n",
        "df_test = pd.read_csv(\"/content/drive/MyDrive/colab_data_files/kaggle_1_dataset/test_dataset.csv\")"
      ],
      "metadata": {
        "id": "vq7__byEHabv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.head()"
      ],
      "metadata": {
        "id": "qWeLFXRQ7NlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's check the distribution of target clas\n",
        "df_train['target'].value_counts()"
      ],
      "metadata": {
        "id": "ET0IBGW3BtDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data is heavily skewed. Only 6% of the samples are toxic/inappropriate"
      ],
      "metadata": {
        "id": "D06os3_qhZiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.isnull().values.any()"
      ],
      "metadata": {
        "id": "M6fvHL16DjCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "no null values exist"
      ],
      "metadata": {
        "id": "bqBbtU0KDuiL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.dtypes"
      ],
      "metadata": {
        "id": "ZMDlrIsoEAJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The columns are in the right format"
      ],
      "metadata": {
        "id": "rVyfB6mjEMHj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.shape"
      ],
      "metadata": {
        "id": "-6ZCiIxxKiq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.shape"
      ],
      "metadata": {
        "id": "fmAg6C3c7DfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.tail()"
      ],
      "metadata": {
        "id": "_Qezmeq67es0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.head()"
      ],
      "metadata": {
        "id": "N5VrG8oQ7nQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oLyIb19KcdL"
      },
      "source": [
        "##   **Stage 2**: Data Pre-Processing  (1 Points)\n",
        "\n",
        "####  Clean and Transform the data into a specified format\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train['question_text'] = df_train['question_text'].apply(lambda x:simple_preprocess(x, max_len=30))"
      ],
      "metadata": {
        "id": "p37H-_qW7m7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test['question_text'] = df_test['question_text'].apply(lambda x:simple_preprocess(x, max_len=30))"
      ],
      "metadata": {
        "id": "4KT2DI9jJo5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.shape"
      ],
      "metadata": {
        "id": "fNxUcXofmjY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.tail()"
      ],
      "metadata": {
        "id": "bLFyrOgHJ8uf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "df_train['question_text'] = df_train['question_text'].apply(lambda x: [w for w in x if not w in stop_words])\n",
        "df_test['question_text'] = df_test['question_text'].apply(lambda x: [w for w in x if not w in stop_words])"
      ],
      "metadata": {
        "id": "4auee0HoKWGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking the max word count\n",
        "tmp_list= [len(i) for i in df_train['question_text']]\n",
        "print(max(tmp_list))"
      ],
      "metadata": {
        "id": "U054XFzTSXlc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(tmp_list)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aV0vKAfCXQ2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.boxplot(tmp_list)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4dSTwTkHX1GQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The maximum length of a question_text is 81. Looking at the box plot, a max seq length of 40 would sufficiently cover all the questions. let's select 40 as our max token count per question_text"
      ],
      "metadata": {
        "id": "SoY_iKZHTgSh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jZg7yL2TtTM"
      },
      "source": [
        "##   **Stage 3**: Build the Word Embeddings using pretrained Word2vec/Glove (Text Representation) (1 Point)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "MAX_SENT_LEN = 40   # Number of words to consider from each question_text\n",
        "MAX_VOCAB_SIZE = 165000   # Max vocabulary size\n",
        "BATCH_SIZE = 128\n",
        "N_EPOCHS = 15"
      ],
      "metadata": {
        "id": "Uj1wOXcF7eB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's do padding to maintain a consistent input length"
      ],
      "metadata": {
        "id": "ewI0sUHdUl0E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)\n",
        "tokenizer.fit_on_texts([' '.join(seq[:MAX_SENT_LEN]) for seq in df_train['question_text']])\n",
        "\n",
        "print(\"Number of words in vocabulary:\", len(tokenizer.word_index))"
      ],
      "metadata": {
        "id": "_9rTpuiwSy0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the sequence of words to sequnce of indices\n",
        "X = tokenizer.texts_to_sequences([' '.join(seq[:MAX_SENT_LEN]) for seq in df_train['question_text']])\n",
        "X = pad_sequences(X, maxlen=MAX_SENT_LEN, padding='post', truncating='post')\n",
        "\n",
        "y = df_train['target']"
      ],
      "metadata": {
        "id": "26z7HC8uV_-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#deleting dataframes to save RAM\n",
        "del df_train"
      ],
      "metadata": {
        "id": "QR0ge6iX_wlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(X[0])"
      ],
      "metadata": {
        "id": "njF4JjP5W0qO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X)"
      ],
      "metadata": {
        "id": "dYiqpDGjWlTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y)"
      ],
      "metadata": {
        "id": "04ytteGCWrte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting the train data into train test split"
      ],
      "metadata": {
        "id": "I-3GCGqqYplh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42, train_size=0.9)"
      ],
      "metadata": {
        "id": "4f-TNq6MYpSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ],
      "metadata": {
        "id": "UEKcClSZZYrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del X"
      ],
      "metadata": {
        "id": "kVajE7OZAHHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's load the 300 dimensional GloVe embedding"
      ],
      "metadata": {
        "id": "rpcoOOD-Zukw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!wget https://nlp.stanford.edu/data/glove.840B.300d.zip\n",
        "#!wget https://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "metadata": {
        "id": "bWPI8Npyt4X6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!unzip glove*.zip"
      ],
      "metadata": {
        "id": "HgY5n03NuRXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''import zipfile\n",
        "zip_ref = zipfile.ZipFile('/content/glove.6B.zip', 'r')\n",
        "zip_ref.extractall('/content/drive/MyDrive/colab_data_files')\n",
        "zip_ref.close()'''"
      ],
      "metadata": {
        "id": "u_kdqtfxwOTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "embeddings_index = {}\n",
        "# Loading the 300-dimensional vector of the model\n",
        "f = open('/content/drive/MyDrive/colab_data_files/glove.6B.300d.txt')\n",
        "count=0\n",
        "for line in f:\n",
        "  try:\n",
        "    #print(line)\n",
        "    print(count)\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "  except:\n",
        "    pass\n",
        "  count+=1\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))\n",
        "'''"
      ],
      "metadata": {
        "id": "_THzeSsRZtrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "'''\n",
        "pickle.dump({'embeddings_index' : embeddings_index } , open('/content/drive/MyDrive/colab_data_files/glove_embeddings_unpacked.pkl', 'wb'))\n",
        "'''"
      ],
      "metadata": {
        "id": "_X3qZR2iZtlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the embedding dictionary already stored as a pickle file. The key Will be a word and values are the embedded vectors as a python arra"
      ],
      "metadata": {
        "id": "JBG5GMedP9y6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_to_read = open(\"/content/drive/MyDrive/colab_data_files/glove_embeddings_unpacked_6b_300d.pkl\", \"rb\")\n",
        "\n",
        "loaded_dict = pickle.load(file_to_read)"
      ],
      "metadata": {
        "id": "P4ZagocKZtaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(loaded_dict)"
      ],
      "metadata": {
        "id": "O9VdQ6SLV8K_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_index=loaded_dict['embeddings_index']"
      ],
      "metadata": {
        "id": "sjSIHFt7WJcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(embeddings_index)"
      ],
      "metadata": {
        "id": "TsFRylevWzwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer"
      ],
      "metadata": {
        "id": "1ubOaQH9RAWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.word_index"
      ],
      "metadata": {
        "id": "zxwrmhfLUbsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(tokenizer.word_index))"
      ],
      "metadata": {
        "id": "atAVU3NYYB-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "creating an embeddings matrix where each row represents a word from the vocabulary we obtained from the training data and the colums represent embedding dimensions"
      ],
      "metadata": {
        "id": "gVu42_f_U2Hr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding 1 because of reversed 0 index\n",
        "words_not_found = []\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Loaded %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "embedding_dim = 300\n",
        "\n",
        "# Create a weight matrix for words in the training data\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "  if i >= vocab_size:\n",
        "    continue\n",
        "  embedding_vector = embeddings_index.get(word)\n",
        "  if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
        "    embedding_matrix[i] = embedding_vector\n",
        "  else:\n",
        "    words_not_found.append(word)"
      ],
      "metadata": {
        "id": "maXGJY4_Uf5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(words_not_found)"
      ],
      "metadata": {
        "id": "zPkfYkxyXFFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "64103 new words out of 160000 are there in the questions that doesnot exist in our embeddings from glove. This seems to be a bad embedding for this problem. But we are not able to use bigger pre trained embeddings due to compute issues"
      ],
      "metadata": {
        "id": "CY_Az4a3XSa-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6jfm3YFUL7i"
      },
      "source": [
        "##   **Stage 4**: Build and Train the Deep networks model using Pytorch/Keras (5 Points)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's first build a Bi directional GRU to establish the baseline"
      ],
      "metadata": {
        "id": "WfDuTiHtYVGu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a sequential model by stacking neural net units\n",
        "model = Sequential()\n",
        "embedding_layer = Embedding(vocab_size,\n",
        "                            embedding_dim,\n",
        "                            weights = [embedding_matrix],\n",
        "                            input_length = MAX_SENT_LEN,\n",
        "                            trainable=False)\n",
        "model.add(embedding_layer)\n",
        "model.add(Bidirectional(GRU(128, return_sequences=True, dropout=0.50, name='first_gru_layer')))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Bidirectional(GRU(64, name='second_gru_layer')))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid', name='output_layer'))"
      ],
      "metadata": {
        "id": "Dz1-Bs4pUdsf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Summary of the built model...')\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "IwzDPJ8bZaTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's train the model"
      ],
      "metadata": {
        "id": "gK6dWS7kZomk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "1QNZz9dLZsu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          epochs=N_EPOCHS,\n",
        "          validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "id": "uJ7FljDvZxN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('/content/drive/MyDrive/colab_data_files/models/bigru_128_64_64.keras')"
      ],
      "metadata": {
        "id": "4Q3bvBCOfPKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-O0Jx99UhmI"
      },
      "source": [
        "##   **Stage 5**: Evaluate the Model and get model predictions on the test dataset (2 Points)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's evaluate the model on test data for accuracy"
      ],
      "metadata": {
        "id": "Sy2QTH6LbIXQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have saved the model for reuse. So loading it here from there."
      ],
      "metadata": {
        "id": "pK7cnKYxy4oD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.load_model('/content/drive/MyDrive/colab_data_files/models/bigru_128_64_64.keras')\n"
      ],
      "metadata": {
        "id": "g6VDER02vLjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Testing...')\n",
        "model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "id": "GJjHpYw3bahy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model predictions on the test data\n",
        "preds = model.predict(X_test)"
      ],
      "metadata": {
        "id": "vTdN5mqhbmcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds.shape"
      ],
      "metadata": {
        "id": "MVGjyIjUbptl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds[:10]"
      ],
      "metadata": {
        "id": "dymUkAIfzna3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the data is highly skewed, the default threshold of 0.5 might not be a right approach to determine the classes from probabilities. There are many ways we could locate the threshold with the optimal balance between false positive and true positive rates.\n",
        "\n",
        "Sensitivity = True Positive Rate\n",
        "Specificity = 1 â€“ False Positive Rate\n",
        "The Geometric Mean or G-Mean is a metric for imbalanced classification that, if optimized, will seek a balance between the sensitivity and the specificity.\n",
        "\n",
        "G-Mean = sqrt(Sensitivity * Specificity)\n",
        "One approach would be to test the model with each threshold returned from the call roc_auc_score() and select the threshold with the largest G-Mean value."
      ],
      "metadata": {
        "id": "i9kxuPRVz6sf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve\n",
        "from numpy import sqrt\n",
        "from numpy import argmax"
      ],
      "metadata": {
        "id": "MjWpzcun12X-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate roc curves\n",
        "fpr, tpr, thresholds = roc_curve(y_test, preds)\n",
        "# plot the roc curve for the model\n",
        "plt.plot([0,1], [0,1], linestyle='--', label='No Skill')\n",
        "plt.plot(fpr, tpr, marker='.', label='Logistic')\n",
        "# axis labels\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.legend()\n",
        "# show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ps_Ea07X1UIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tpr"
      ],
      "metadata": {
        "id": "3dl1eAn82bwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fpr"
      ],
      "metadata": {
        "id": "ASCwIh1j2e02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate the g-mean for each threshold\n",
        "gmeans = sqrt(tpr * (1-fpr))"
      ],
      "metadata": {
        "id": "rLzi0iOK1sbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "let's look at the scwtter plot of threshold vs gmeans"
      ],
      "metadata": {
        "id": "77Db8uGT3R2d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(thresholds, gmeans)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ra7SAZxP3HX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# locate the index of the largest g-mean\n",
        "ix = argmax(gmeans)\n",
        "print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))"
      ],
      "metadata": {
        "id": "zSgbMimX29lC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Though, the best value of threshold is 0.047 which gives the highest gmean on test dataset, the curve is not smooth near 0, so it would be a prudent approach to go with a threshold in the smoother part of the curve. Lets do the same analysis on y_train which is a larger dataset."
      ],
      "metadata": {
        "id": "D9BJDeKw3y8h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preds_train = model.predict(X_train)"
      ],
      "metadata": {
        "id": "3_1MrSQG7YHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate roc curves\n",
        "fpr, tpr, thresholds = roc_curve(y_train, preds_train)\n",
        "# plot the roc curve for the model\n",
        "plt.plot([0,1], [0,1], linestyle='--', label='No Skill')\n",
        "plt.plot(fpr, tpr, marker='.', label='Logistic')\n",
        "# axis labels\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.legend()\n",
        "# show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9K293UNa7YEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate the g-mean for each threshold\n",
        "gmeans = sqrt(tpr * (1-fpr))"
      ],
      "metadata": {
        "id": "U5HBsc7O72Th"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(thresholds, gmeans)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3xAZYQbK74Nn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# locate the index of the largest g-mean\n",
        "ix = argmax(gmeans)\n",
        "print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))"
      ],
      "metadata": {
        "id": "M0T6Ny9-8Cco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we used a threshold of 0.15 which seemed to be the best from the scatter plot, the score on kaggle public dataset decreased. Probably, gmean is not a good metric. Let's use f1 score as our metric directly to perform a grid search over the threshold values."
      ],
      "metadata": {
        "id": "HYnBsPgH8rUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's get the list of threshold values to search over\n",
        "t = [round(i,1) for i in thresholds]\n",
        "t =list(set(t))\n",
        "t =[i for i in t if i<=1]"
      ],
      "metadata": {
        "id": "L1uFiJJ_9cj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t"
      ],
      "metadata": {
        "id": "gL7mEG6a-D7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1scores=[]\n",
        "for threshold in t:\n",
        "  y_pred = [1 if i>threshold else 0 for i in preds]\n",
        "  f1scores.append(f1_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "RmIEW0Ua-aFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1scores"
      ],
      "metadata": {
        "id": "6V-ytl0RApoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's check the same on train data\n",
        "f1scores=[]\n",
        "for threshold in t:\n",
        "  y_pred = [1 if i>threshold else 0 for i in preds_train]\n",
        "  f1scores.append(f1_score(y_train, y_pred))"
      ],
      "metadata": {
        "id": "ma5ItLVwA8es"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1scores"
      ],
      "metadata": {
        "id": "DBFVisRJBMWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A threshold value of 0.3 seems to be the optimum cutoff for the best f1 score on both test and train dataset"
      ],
      "metadata": {
        "id": "aQGOR-gcBYn7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's use the learnt model to make predictions on df_test"
      ],
      "metadata": {
        "id": "5EcrDbLYdCk0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get the predictions\n",
        "def predictions(df_test, threshold):\n",
        "\n",
        "  questions_list_idx = tokenizer.texts_to_sequences([' '.join(seq[:MAX_SENT_LEN]) for seq in df_test['question_text']])\n",
        "\n",
        "  # Pad the sequences of the data\n",
        "  questions_list_idx = pad_sequences(questions_list_idx, maxlen=MAX_SENT_LEN, padding='post', truncating='post')\n",
        "\n",
        "  # Get the predictons by using GRU model\n",
        "  review_preds = model.predict(questions_list_idx)\n",
        "\n",
        "  # Add the predictions to the movie reviews data\n",
        "  df_test['predictions'] = review_preds\n",
        "\n",
        "  # Set the threshold for the predictions\n",
        "  pred_sentiment = np.array(list(map(lambda x : 1 if x > threshold else 0, review_preds)))\n",
        "\n",
        "  # Add the sentiment predictions to the movie reviews\n",
        "  df_test['predicted target'] = pred_sentiment\n",
        "\n",
        "  return df_test"
      ],
      "metadata": {
        "id": "NpBNORyCc74a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#df_test = predictions(df_test, 0.15)"
      ],
      "metadata": {
        "id": "AKxgAWMbe61L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = predictions(df_test, 0.3)"
      ],
      "metadata": {
        "id": "jXQ8SmFXBvfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.shape"
      ],
      "metadata": {
        "id": "do4xrflNniH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.to_csv('/content/drive/MyDrive/colab_data_files/results/df_test_3.csv', index=False)"
      ],
      "metadata": {
        "id": "Wk0KP5bXe6uU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.head()"
      ],
      "metadata": {
        "id": "HwK2jN8Fe6hA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def df_test_to_submission(df_test):\n",
        "  result=df_test.rename(columns = {'predicted target':'target'})\n",
        "  result= result[['qid', 'target']]\n",
        "  return result"
      ],
      "metadata": {
        "id": "CbF7PBCOjUKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission = df_test_to_submission(df_test)"
      ],
      "metadata": {
        "id": "G0A6dHZHkHzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission.shape"
      ],
      "metadata": {
        "id": "4nbRJ63soNqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission.head()"
      ],
      "metadata": {
        "id": "qhF3y5YbkVab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some issueexists in the submission file in kaggle. There are 23 qids which does not exist in test datasetbut are expected in the sample submission files. so we are going to default these 23 entries to the target value of '0' to make valid submission"
      ],
      "metadata": {
        "id": "waXGNpx5yOjj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_df = pd.read_csv('/content/drive/MyDrive/colab_data_files/kaggle_1_dataset/sample_submission.csv')"
      ],
      "metadata": {
        "id": "rKuzlkvdtbyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sub_df = pd.read_csv('/content/drive/MyDrive/colab_data_files/kaggle_1_dataset/test_dataset.csv')"
      ],
      "metadata": {
        "id": "gdQ8CQTHt7xp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_df.shape"
      ],
      "metadata": {
        "id": "BgLOB_snuGvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "problematic_qids =list(set(sample_df['qid'].values)-set(sub_df['qid'].values))"
      ],
      "metadata": {
        "id": "rgQxpkS_uVRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "problematic_qids"
      ],
      "metadata": {
        "id": "UFSXttl4uz5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(problematic_qids)"
      ],
      "metadata": {
        "id": "pDUBqPthu3Fe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission.shape"
      ],
      "metadata": {
        "id": "nKEpC0GB0e-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for qid in problematic_qids:\n",
        "  row = pd.Series([qid, 0], index = submission.columns)\n",
        "  submission = submission.append(row, ignore_index=True)"
      ],
      "metadata": {
        "id": "8OC977ZxzRPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission.shape"
      ],
      "metadata": {
        "id": "IAjKWzdQ1Cf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's remove those entries in submission which are not there in sample submission file\n",
        "extra_qids =list(set(submission['qid'].values)-set(sample_df['qid'].values))\n",
        "len(extra_qids)\n"
      ],
      "metadata": {
        "id": "X3HsfEvy1tIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission =submission[submission['qid'].isin(list(sample_df['qid'].values))]"
      ],
      "metadata": {
        "id": "neYAB2hH2d5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission.shape"
      ],
      "metadata": {
        "id": "1cEBx4vF3PoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission.to_csv('/content/drive/MyDrive/colab_data_files/results/submission_3.csv', index=False)"
      ],
      "metadata": {
        "id": "Jp7bZtMYkiLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Lets train an LSTM to see if the performance improves***"
      ],
      "metadata": {
        "id": "JDRWXMuCFRqD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will not remove stopwords now from the question text in the LSTM model. Since the problem is to find toxic/inappropriate questions, words like 'is', 'what', 'why' might be useful ."
      ],
      "metadata": {
        "id": "WikEjQzoFrK3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pd.read_csv(\"/content/drive/MyDrive/colab_data_files/kaggle_1_dataset/train_dataset.csv\")\n",
        "df_test = pd.read_csv(\"/content/drive/MyDrive/colab_data_files/kaggle_1_dataset/test_dataset.csv\")"
      ],
      "metadata": {
        "id": "1zq55hu4Fjsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train['question_text'] = df_train['question_text'].apply(lambda x:simple_preprocess(x, max_len=30))\n",
        "df_test['question_text'] = df_test['question_text'].apply(lambda x:simple_preprocess(x, max_len=30))\n"
      ],
      "metadata": {
        "id": "OE2BL-nIHjLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "MAX_SENT_LEN = 40   # Number of words to consider from each question_text\n",
        "MAX_VOCAB_SIZE = 165000   # Max vocabulary size\n",
        "BATCH_SIZE = 128\n",
        "N_EPOCHS = 15"
      ],
      "metadata": {
        "id": "j3TqiiqaH4C6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's do padding to maintain a consistent input length\n",
        "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)\n",
        "tokenizer.fit_on_texts([' '.join(seq[:MAX_SENT_LEN]) for seq in df_train['question_text']])\n",
        "\n",
        "print(\"Number of words in vocabulary:\", len(tokenizer.word_index))\n",
        "\n",
        "# Convert the sequence of words to sequnce of indices\n",
        "X = tokenizer.texts_to_sequences([' '.join(seq[:MAX_SENT_LEN]) for seq in df_train['question_text']])\n",
        "X = pad_sequences(X, maxlen=MAX_SENT_LEN, padding='post', truncating='post')\n",
        "\n",
        "y = df_train['target']\n"
      ],
      "metadata": {
        "id": "HldcyYfbH3-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42, train_size=0.9)"
      ],
      "metadata": {
        "id": "XdqM8qYTH3l-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's load the 300 dimensional GloVe embedding\n",
        "import pickle\n",
        "file_to_read = open(\"/content/drive/MyDrive/colab_data_files/glove_embeddings_unpacked_6b_300d.pkl\", \"rb\")\n",
        "\n",
        "loaded_dict = pickle.load(file_to_read)\n",
        "embeddings_index=loaded_dict['embeddings_index']\n",
        "\n"
      ],
      "metadata": {
        "id": "MtLfuYNkIeCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating an embeddings matrix where each row represents a word from the vocabulary we obtained from the training data and the colums represent embedding dimensions\n",
        "# Adding 1 because of reversed 0 index\n",
        "words_not_found = []\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Loaded %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "embedding_dim = 300\n",
        "\n",
        "# Create a weight matrix for words in the training data\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "  if i >= vocab_size:\n",
        "    continue\n",
        "  embedding_vector = embeddings_index.get(word)\n",
        "  if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
        "    embedding_matrix[i] = embedding_vector\n",
        "  else:\n",
        "    words_not_found.append(word)\n",
        "\n"
      ],
      "metadata": {
        "id": "kLv3JtweId-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(words_not_found)"
      ],
      "metadata": {
        "id": "pDRdSxsxId5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a sequential model by stacking neural net units\n",
        "model2 = Sequential()\n",
        "embedding_layer = Embedding(vocab_size,\n",
        "                            embedding_dim,\n",
        "                            weights = [embedding_matrix],\n",
        "                            input_length = MAX_SENT_LEN,\n",
        "                            trainable=False)\n",
        "model2.add(embedding_layer)\n",
        "model2.add(Bidirectional(LSTM(128, return_sequences=True, dropout=0.50, name='first_lstm_layer')))\n",
        "model2.add(Dropout(0.5))\n",
        "model2.add(Bidirectional(LSTM(64, name='second_lstm_layer')))\n",
        "model2.add(Dropout(0.5))\n",
        "model2.add(Dense(64, activation='relu'))\n",
        "model2.add(Dropout(0.5))\n",
        "model2.add(Dense(1, activation='sigmoid', name='output_layer'))"
      ],
      "metadata": {
        "id": "52BHl8C5JUmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Summary of the built model...')\n",
        "model2.summary()"
      ],
      "metadata": {
        "id": "n3GwqU4BJUie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "vBuyJXSVJUfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2.fit(X_train, y_train,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          epochs=N_EPOCHS,\n",
        "          validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "id": "1fNXifzzJUZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2.save('/content/drive/MyDrive/colab_data_files/models/bilstm_128_64_64.keras')"
      ],
      "metadata": {
        "id": "K4zkbAWeKg0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have saved the model for re-use. So loading it from there."
      ],
      "metadata": {
        "id": "i1Lq8cOq4TPn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = tf.keras.models.load_model('/content/drive/MyDrive/colab_data_files/models/bilstm_128_64_64.keras')\n"
      ],
      "metadata": {
        "id": "JByC0Bj5Kgr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model predictions on the test data\n",
        "preds = model2.predict(X_test)\n"
      ],
      "metadata": {
        "id": "txwHbX46K2Mh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve\n",
        "from numpy import sqrt\n",
        "from numpy import argmax"
      ],
      "metadata": {
        "id": "ZeSBXikmWr5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t=[0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]"
      ],
      "metadata": {
        "id": "Sg4xVMfKWvP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1scores=[]\n",
        "for threshold in t:\n",
        "  y_pred = [1 if i>threshold else 0 for i in preds]\n",
        "  f1scores.append(f1_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "x_-SQHNaXEvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1scores"
      ],
      "metadata": {
        "id": "5E8NANkkXEaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model predictions on the test data\n",
        "preds_train = model2.predict(X_train)\n"
      ],
      "metadata": {
        "id": "SiN-FEOQXgxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's check the same on train data\n",
        "f1scores=[]\n",
        "for threshold in t:\n",
        "  y_pred = [1 if i>threshold else 0 for i in preds_train]\n",
        "  f1scores.append(f1_score(y_train, y_pred))"
      ],
      "metadata": {
        "id": "FchjcZhXXEXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1scores"
      ],
      "metadata": {
        "id": "D5SNlCSLXgqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A threshold value of 0.5 seems to be the optimum cutoff for the best f1 score on both test and train dataset"
      ],
      "metadata": {
        "id": "Kg5ZgpdvXzji"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's use the learnt model to make predictions on df_test"
      ],
      "metadata": {
        "id": "fRieDv2yX_R1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get the predictions\n",
        "def predictions(df_test, threshold):\n",
        "\n",
        "  questions_list_idx = tokenizer.texts_to_sequences([' '.join(seq[:MAX_SENT_LEN]) for seq in df_test['question_text']])\n",
        "\n",
        "  # Pad the sequences of the data\n",
        "  questions_list_idx = pad_sequences(questions_list_idx, maxlen=MAX_SENT_LEN, padding='post', truncating='post')\n",
        "\n",
        "  # Get the predictons by using GRU model\n",
        "  review_preds = model2.predict(questions_list_idx)\n",
        "\n",
        "  # Add the predictions to the movie reviews data\n",
        "  df_test['predictions'] = review_preds\n",
        "\n",
        "  # Set the threshold for the predictions\n",
        "  pred_sentiment = np.array(list(map(lambda x : 1 if x > threshold else 0, review_preds)))\n",
        "\n",
        "  # Add the sentiment predictions to the movie reviews\n",
        "  df_test['predicted target'] = pred_sentiment\n",
        "\n",
        "  return df_test"
      ],
      "metadata": {
        "id": "u1IADjRXXgmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = predictions(df_test, 0.5)\n",
        "df_test.to_csv('/content/drive/MyDrive/colab_data_files/results/df_test_4.csv', index=False)"
      ],
      "metadata": {
        "id": "rrfyMP2xXgg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def df_test_to_submission(df_test):\n",
        "  result=df_test.rename(columns = {'predicted target':'target'})\n",
        "  result= result[['qid', 'target']]\n",
        "  return result"
      ],
      "metadata": {
        "id": "AdHJ5YlnYTwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission = df_test_to_submission(df_test)"
      ],
      "metadata": {
        "id": "QRiAXtXvYYA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some issue exists in the submission file in kaggle. There are 23 qids which does not exist in test dataset but are expected in the sample submission files. so we are going to default these 23 entries to the target value of '0' to make valid submission"
      ],
      "metadata": {
        "id": "EbGGdqmJYe8X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_df = pd.read_csv('/content/drive/MyDrive/colab_data_files/kaggle_1_dataset/sample_submission.csv')\n",
        "sub_df = pd.read_csv('/content/drive/MyDrive/colab_data_files/kaggle_1_dataset/test_dataset.csv')\n",
        "problematic_qids =list(set(sample_df['qid'].values)-set(sub_df['qid'].values))\n",
        "print(len(problematic_qids))\n",
        "for qid in problematic_qids:\n",
        "  row = pd.Series([qid, 0], index = submission.columns)\n",
        "  submission = submission.append(row, ignore_index=True)\n",
        "\n",
        "submission =submission[submission['qid'].isin(list(sample_df['qid'].values))]\n",
        "print(submission.shape)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "D6iFyaykYlkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission.to_csv('/content/drive/MyDrive/colab_data_files/results/submission_4.csv', index=False)"
      ],
      "metadata": {
        "id": "4tm_BySJZE9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **The above file is our final version of submission. We could not complete BERT training due to copute issues**"
      ],
      "metadata": {
        "id": "ZUboRJsf6XQZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Lets train a BERT model to see if the accuracy improves further***"
      ],
      "metadata": {
        "id": "NKJuwLPkbhOv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install transformers"
      ],
      "metadata": {
        "id": "yvyyHkWqbzgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "njldMqUKbg3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pd.read_csv(\"/content/drive/MyDrive/colab_data_files/kaggle_1_dataset/train_dataset.csv\")\n",
        "df_test = pd.read_csv(\"/content/drive/MyDrive/colab_data_files/kaggle_1_dataset/test_dataset.csv\")\n",
        "\n",
        "tr_texts = df_train['question_text'].to_list()\n",
        "te_texts = df_test['question_text'].to_list()\n",
        "\n",
        "tr_labels=df_train['target'].to_list()\n"
      ],
      "metadata": {
        "id": "V8hgmU9IbgzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Lets Create a custom dataset class for text classification\n",
        "class TextClassificationDataset(Dataset):\n",
        "  def __init__(self, texts, labels, tokenizer, max_length):\n",
        "    self.texts = texts\n",
        "    self.labels = labels\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_length = max_length\n",
        "  def __len__(self):\n",
        "    return len(self.texts)\n",
        "  def __getitem__(self, idx):\n",
        "    text = self.texts[idx]\n",
        "    label = self.labels[idx]\n",
        "    encoding = self.tokenizer(text, return_tensors='pt', max_length=self.max_length, padding='max_length', truncation=True)\n",
        "    return {'input_ids': encoding['input_ids'].flatten(), 'attention_mask': encoding['attention_mask'].flatten(), 'label': torch.tensor(label)}\n",
        "\n"
      ],
      "metadata": {
        "id": "8EJDngv4bgvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Build our customer BERT classifier\n",
        "class BERTClassifier(nn.Module):\n",
        "  def __init__(self, bert_model_name, num_classes):\n",
        "    super(BERTClassifier, self).__init__()\n",
        "    self.bert = BertModel.from_pretrained(bert_model_name)\n",
        "    self.dropout = nn.Dropout(0.1)\n",
        "    self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
        "\n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    pooled_output = outputs.pooler_output\n",
        "    x = self.dropout(pooled_output)\n",
        "    logits = self.fc(x)\n",
        "    return logits"
      ],
      "metadata": {
        "id": "Qz4DqFxUbgru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the train() function\n",
        "def train(model, data_loader, optimizer, scheduler, device):\n",
        "  model.train()\n",
        "  for batch in data_loader:\n",
        "    optimizer.zero_grad()\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    labels = batch['label'].to(device)\n",
        "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()"
      ],
      "metadata": {
        "id": "idX5KH89bgod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build our evaluation method\n",
        "def evaluate(model, data_loader, device):\n",
        "  model.eval()\n",
        "  predictions = []\n",
        "  actual_labels = []\n",
        "  with torch.no_grad():\n",
        "    for batch in data_loader:\n",
        "      input_ids = batch['input_ids'].to(device)\n",
        "      attention_mask = batch['attention_mask'].to(device)\n",
        "      labels = batch['label'].to(device)\n",
        "      outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "      predictions.extend(preds.cpu().tolist())\n",
        "      actual_labels.extend(labels.cpu().tolist())\n",
        "  return accuracy_score(actual_labels, predictions), classification_report(actual_labels, predictions)"
      ],
      "metadata": {
        "id": "6pfXdBXqbglR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Build our prediction method\n",
        "def predict_toxicity(text, model, tokenizer, device, max_length=128):\n",
        "  model.eval()\n",
        "  encoding = tokenizer(text, return_tensors='pt', max_length=max_length, padding='max_length', truncation=True)\n",
        "  input_ids = encoding['input_ids'].to(device)\n",
        "  attention_mask = encoding['attention_mask'].to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "  return 1 if preds.item() == 1 else 0\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4pQXR5xObggU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define our modelâ€™s parameters\n",
        "bert_model_name = 'bert-base-uncased'\n",
        "num_classes = 2\n",
        "max_length = 128\n",
        "batch_size = 40\n",
        "num_epochs = 5\n",
        "learning_rate = 2e-5\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3fNZ8pHwbgdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading and splitting the data.\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(tr_texts, tr_labels, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "iRTNlOG-bgYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialize tokenizer, dataset, and data loader\n",
        "tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
        "train_dataset = TextClassificationDataset(train_texts, train_labels, tokenizer, max_length)\n",
        "val_dataset = TextClassificationDataset(val_texts, val_labels, tokenizer, max_length)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n"
      ],
      "metadata": {
        "id": "SJg0YI9IbgVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = BERTClassifier(bert_model_name, num_classes).to(device)"
      ],
      "metadata": {
        "id": "laJlj_robgAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Set up optimizer and learning rate scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "total_steps = len(train_dataloader) * num_epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)"
      ],
      "metadata": {
        "id": "hQoBXjBFlXkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training the model\n",
        "for epoch in range(num_epochs):\n",
        "  print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "  train(model, train_dataloader, optimizer, scheduler, device)\n",
        "  accuracy, report = evaluate(model, val_dataloader, device)\n",
        "  print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
        "  print(report)\n"
      ],
      "metadata": {
        "id": "1MqemF8HlXgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is failing in training because it takes a lot of time to train. The compute provided by collab stops running midway."
      ],
      "metadata": {
        "id": "2auuCEjezMS1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"bert_classifier.pth\")"
      ],
      "metadata": {
        "id": "WNGRt1namBAe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}